import os
import math
from typing import List
from dotenv import load_dotenv

# LangChain ë° OpenAI ë„êµ¬ë“¤
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_core.runnables import RunnableLambda

# 1. API í‚¤ í™•ì¸
load_dotenv()
if not os.getenv("OPENAI_API_KEY"):
    print("âŒ ì˜¤ë¥˜: .env íŒŒì¼ì— OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
    exit()

print(">>> [ì‹œìŠ¤í…œ] OpenAI ì±—ë´‡ ì‹œë™ ì¤‘... (Custom Lite Ver.)")

# 2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
try:
    loader = TextLoader("./data/pregnancy_guide.txt", encoding="utf-8")
    docs = loader.load()
except Exception as e:
    print("âŒ ./data/pregnancy_guide.txt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# 3. í…ìŠ¤íŠ¸ ìª¼ê°œê¸°
text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
splits = text_splitter.split_documents(docs)

# 4. [í•µì‹¬] ì§ì ‘ ë§Œë“  ê°„ì´ ê²€ìƒ‰ê¸° (LiteRetriever)
# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ì— ìƒê´€ì—†ì´ ì‘ë™í•˜ë„ë¡ ì§ì ‘ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.
class LiteRetriever:
    def __init__(self, documents, embedding_model):
        self.documents = documents
        self.embedding_model = embedding_model
        print(">>> ë°ì´í„° í•™ìŠµ ì¤‘... (ë‹¨ìˆœ ê³„ì‚°)")
        # ë¯¸ë¦¬ í…ìŠ¤íŠ¸ë“¤ì„ ë²¡í„°(ìˆ«ì)ë¡œ ë³€í™˜í•´ë‘¡ë‹ˆë‹¤.
        texts = [d.page_content for d in documents]
        self.doc_vectors = embedding_model.embed_documents(texts)

    def similarity_search(self, query: str, k: int = 3):
        # ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
        query_vec = self.embedding_model.embed_query(query)
        
        # ëª¨ë“  ë¬¸ì„œì™€ ìœ ì‚¬ë„ ë¹„êµ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°)
        scores = []
        for i, doc_vec in enumerate(self.doc_vectors):
            # ë‚´ì (Dot Product) ê³„ì‚°
            score = sum(q * d for q, d in zip(query_vec, doc_vec))
            scores.append((score, self.documents[i]))
        
        # ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•´ì„œ kê°œ ë°˜í™˜
        scores.sort(key=lambda x: x[0], reverse=True)
        return [doc for score, doc in scores[:k]]

    def as_retriever(self):
        # ë­ì²´ì¸ ì²´ì¸ì— ì—°ê²°í•˜ê¸° ìœ„í•œ ê»ë°ê¸° í•¨ìˆ˜
        return RunnableLambda(lambda x: self.similarity_search(x["input"] if isinstance(x, dict) else x))

# ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# ë‚´ìˆ˜ìš© ê²€ìƒ‰ê¸° ìƒì„± (ì—¬ê¸°ì„œ ì—ëŸ¬ê°€ ì•ˆ ë‚©ë‹ˆë‹¤)
custom_retriever = LiteRetriever(splits, embeddings).as_retriever()

# 5. ì±—ë´‡ ëª¨ë¸ ì„¤ì •
llm = ChatOpenAI(model="gpt-4o")

# 6. í”„ë¡¬í”„íŠ¸ ë° ì²´ì¸ ì—°ê²°
system_prompt = (
    "ë‹¹ì‹ ì€ ë‚œì„ ë° ì„ì‹  ì¤€ë¹„ ì—¬ì„±ì„ ë•ëŠ” ë”°ëœ»í•˜ê³  ì‚¬ë ¤ ê¹Šì€ 'AI ì½”ë””ë„¤ì´í„°'ì…ë‹ˆë‹¤. "
    "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì•„ë˜ **ìš°ì„ ìˆœìœ„ ì§€ì¹¨**ì— ë”°ë¼ ë‹µë³€í•˜ì„¸ìš”.\n\n"
    
    "**[ì œ1ì›ì¹™: ì˜ë£Œ ì•ˆì „]**\n"
    "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ 'í†µì¦', 'ì¶œí˜ˆ', 'ê³ ì—´', 'ë³µí†µ', 'ì‹¬ê°í•œ ë¶€ì‘ìš©' ë“± ì˜í•™ì  ìœ„ê¸‰ìƒí™©ì´ë‚˜ "
    "ê°œì¸ì ì¸ ì§„ë‹¨(ì˜ˆ: 'ì´ê±° ìœ ì‚°ì¸ê°€ìš”?', 'ì´ ì•½ ë¨¹ì–´ë„ ë ê¹Œìš”?')ì„ ìš”êµ¬í•˜ëŠ” ë‰˜ì•™ìŠ¤ê°€ ëŠê»´ì§„ë‹¤ë©´, "
    "ì–´ë–¤ ì •ë³´ë„ ì£¼ì§€ ë§ê³  **'ì €ëŠ” ì˜ì‚¬ê°€ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ì •í™•í•œ ì§„ë‹¨ì„ ìœ„í•´ ê¼­ ë³‘ì›ì— ë°©ë¬¸í•˜ì…”ì„œ ì „ë¬¸ì˜ì™€ ìƒë‹´í•´ë³´ì‹œê¸¸ ê¶Œí•´ë“œë ¤ìš”.'**ë¼ê³  ì •ì¤‘í•˜ê²Œ ì•ˆë‚´í•˜ì„¸ìš”.\n\n"
    
    "**[ì œ2ì›ì¹™: ì„ì‹ /ì˜ì–‘ì œ ì •ë³´]**\n"
    "ì œ1ì›ì¹™ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ì„ì‹ , ì˜ì–‘ì œ, ì‹œìˆ  ê´€ë ¨ ì •ë³´ì„± ì§ˆë¬¸ì€ "
    "ë°˜ë“œì‹œ ì•„ë˜ ì œê³µëœ **[ë¬¸ë§¥]**ì— ìˆëŠ” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”. "
    "[ë¬¸ë§¥]ì— ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ê³  ì†”ì§í•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”.\n\n"
    
    "**[ì œ3ì›ì¹™: ì¼ìƒ ëŒ€í™”]**\n"
    "ì„ì‹ ê³¼ ê´€ë ¨ ì—†ëŠ” ì¼ìƒì ì¸ ì£¼ì œ(ì¸ì‚¬, ë‚ ì”¨, ë©”ë‰´ ì¶”ì²œ, ìœ„ë¡œ ë“±)ëŠ” "
    "ë‹¹ì‹ ì˜ ì¼ë°˜ì ì¸ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ì¹œêµ¬ì²˜ëŸ¼ ììœ ë¡­ê³  ì¹œì ˆí•˜ê²Œ ëŒ€í™”í•˜ì„¸ìš”.\n\n"
    
    "**[ë§íˆ¬ ê°€ì´ë“œ]**\n"
    "- í•­ìƒ ì‚¬ìš©ìì˜ í˜ë“  ë§ˆìŒì— ê³µê°í•˜ëŠ” ë”°ëœ»í•˜ê³  ë¶€ë“œëŸ¬ìš´ 'í•´ìš”'ì²´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n"
    "- ë”±ë”±í•œ ì„¤ëª…ë³´ë‹¤ëŠ” ì˜†ì—ì„œ ì±™ê²¨ì£¼ëŠ” ì–¸ë‹ˆë‚˜ ì¹œêµ¬ ê°™ì€ ëŠë‚Œì„ ì£¼ì„¸ìš”.\n\n"
    
    "[ë¬¸ë§¥]:\n{context}"
)

prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "{input}"),
])

question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(custom_retriever, question_answer_chain)

# 7. ì‹¤í–‰
print("\n" + "="*40)
print("âœ… ì±—ë´‡ ì¤€ë¹„ ì™„ë£Œ (ì¢…ë£Œ: exit)")
print("="*40)

while True:
    user_input = input("\nğŸ‘¤ ì§ˆë¬¸: ")
    if user_input.lower() in ["exit", "ì¢…ë£Œ"]:
        break
    
    try:
        response = rag_chain.invoke({"input": user_input})
        print(f"ğŸ¤– ë‹µë³€: {response['answer']}")
    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")